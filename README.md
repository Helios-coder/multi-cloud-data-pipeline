# ğŸš€ Multi-Cloud Data Pipeline Framework

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5+-orange.svg)](https://spark.apache.org/)
[![Azure](https://img.shields.io/badge/Azure-Supported-0089D6.svg)](https://azure.microsoft.com/)
[![GCP](https://img.shields.io/badge/GCP-Supported-4285F4.svg)](https://cloud.google.com/)

A production-ready, cloud-agnostic data pipeline framework that works seamlessly across **Azure** and **Google Cloud Platform**. Built with PySpark for scalable data engineering.

## ğŸ¯ Overview

This framework provides a unified abstraction layer for building modern data pipelines that can run on both Azure and GCP with minimal code changes. It supports batch processing, real-time streaming, data quality validation, and automated orchestration.

### Key Features

- ğŸ”„ **Multi-Cloud Support**: Single codebase for Azure (Databricks, Synapse, Data Lake) and GCP (BigQuery, Dataflow, Cloud Storage)
- âš¡ **PySpark Native**: Optimized transformations using Apache Spark for big data processing
- ğŸ”Œ **Flexible Connectors**: Pre-built connectors for common data sources (databases, APIs, streaming)
- ğŸ“Š **Data Quality**: Built-in validation framework with Great Expectations integration
- ğŸ­ **Orchestration Ready**: Compatible with Airflow, Prefect, Azure Data Factory, Cloud Composer
- ğŸ” **Security First**: Encryption, RBAC, and audit logging included
- ğŸ“ˆ **Performance Optimized**: Intelligent partitioning, caching, and query optimization
- ğŸ—ï¸ **Infrastructure as Code**: Terraform modules for both clouds

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Sources                              â”‚
â”‚  (Databases, APIs, Files, Streaming Sources)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Ingestion Layer                              â”‚
â”‚  â€¢ Batch Connectors  â€¢ Streaming Connectors  â€¢ API Adapters â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Processing Layer (PySpark)                      â”‚
â”‚  â€¢ Transformations  â€¢ Data Quality  â€¢ Schema Evolution      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Storage Layer                                â”‚
â”‚  Azure: Data Lake, Synapse  â”‚  GCP: BigQuery, GCS          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Orchestration & Monitoring                         â”‚
â”‚  â€¢ Airflow/Prefect  â€¢ Metadata Catalog  â€¢ Lineage Tracking â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Basic Usage

```python
from multicloud_pipeline import Pipeline, AzureConnector, SparkTransformer

# Create a pipeline
pipeline = Pipeline(
    name="sales_data_pipeline",
    cloud_provider="azure"  # or "gcp"
)

# Add source connector
source = AzureConnector(
    connection_type="blob_storage",
    container="raw-data",
    path="sales/*.parquet"
)

# Add transformation
transformer = SparkTransformer(
    transformation_type="aggregate",
    group_by=["product_id", "date"],
    aggregations={"revenue": "sum", "quantity": "sum"}
)

# Execute pipeline
pipeline.add_source(source)
pipeline.add_transformer(transformer)
pipeline.run()
```

## ğŸ“š Examples

Check the `/examples` directory for complete use cases:

- **Batch ETL Pipeline**: Daily sales data processing
- **Real-Time Streaming**: Event processing with Kafka/Pub/Sub
- **Multi-Cloud Migration**: Azure to GCP data transfer
- **ML Feature Engineering**: Feature store integration

## ğŸ› ï¸ Tech Stack

| Category | Technologies |
|----------|-------------|
| **Cloud Platforms** | Azure (Databricks, Synapse, Data Lake, Data Factory) <br> GCP (BigQuery, Dataflow, Cloud Storage, Pub/Sub) |
| **Processing** | Apache Spark 3.5+, PySpark, Databricks Runtime |
| **Storage** | Azure Data Lake Gen2, Azure Synapse, Google BigQuery, Cloud Storage |
| **Streaming** | Apache Kafka, Azure Event Hubs, Google Pub/Sub |
| **Orchestration** | Apache Airflow, Prefect, Azure Data Factory, Cloud Composer |
| **Data Quality** | Great Expectations, Custom validators |
| **IaC** | Terraform, ARM Templates |
| **CI/CD** | GitHub Actions |

## ğŸ“‚ Project Structure

```
multi-cloud-data-pipeline/
â”œâ”€â”€ src/multicloud_pipeline/     # Core framework
â”‚   â”œâ”€â”€ connectors/              # Data connectors
â”‚   â”œâ”€â”€ transformers/            # PySpark transformations
â”‚   â”œâ”€â”€ orchestration/           # Pipeline orchestration
â”‚   â”œâ”€â”€ quality/                 # Data quality checks
â”‚   â””â”€â”€ utils/                   # Utilities
â”œâ”€â”€ terraform/                   # Infrastructure as Code
â”‚   â”œâ”€â”€ azure/                   # Azure resources
â”‚   â””â”€â”€ gcp/                     # GCP resources
â”œâ”€â”€ examples/                    # Example pipelines
â”œâ”€â”€ tests/                       # Unit and integration tests
â”œâ”€â”€ docs/                        # Documentation
â””â”€â”€ .github/workflows/           # CI/CD pipelines
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest --cov=multicloud_pipeline tests/
```

## ğŸ“Š Performance Benchmarks

| Operation | Azure Databricks | GCP Dataflow | Optimization |
|-----------|-----------------|--------------|--------------|
| 100GB Parquet Ingestion | 45s | 52s | Partitioning |
| Complex Aggregation (1TB) | 3m 20s | 3m 45s | Broadcast joins |
| Streaming (10K events/s) | 120ms latency | 140ms latency | Micro-batching |

## ğŸ¤ Contributing

Contributions are welcome! Please read our [Contributing Guide](CONTRIBUTING.md) for details.

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“§ Contact

Created by **Alexandre** - Data Engineer specializing in cloud-native data platforms

- GitHub: [@AlexandreFCosta](https://github.com/AlexandreFCosta)
- LinkedIn: [AlexandreCosta ](https://www.linkedin.com/in/alexandrefeitosacosta/)

---

â­ If you find this project useful, please consider giving it a star!

**Tags**: `data-engineering` `azure` `gcp` `pyspark` `databricks` `bigquery` `etl` `data-pipeline` `cloud` `spark`
